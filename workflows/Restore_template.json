{
  "3": {
    "inputs": {
      "seed": "__SEED__",
      "steps": "__STEPS__",
      "cfg": "__CFG__",
      "sampler_name": "dpmpp_2s_ancestral_cfg_pp",
      "scheduler": "karras",
      "denoise": 1,
      "model": [
        "14",
        0
      ],
      "positive": [
        "32",
        0
      ],
      "negative": [
        "32",
        1
      ],
      "latent_image": [
        "12",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "6": {
    "inputs": {
      "text": [
        "60",
        0
      ],
      "clip": [
        "14",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "7": {
    "inputs": {
      "text": "Negative(hands), text, error, cropped, (worst quality:1.2), (low quality:1.2), normal quality, (jpeg artifacts:1.3), signature, watermark, username, blurry, artist name, monochrome, sketch, censorship, censor, (copyright:1.2), extra legs,(forehead mark),(depth of field),(emotionless),(penis), noises, dusts, scratches, multicolor, patches, extra hands, wrong adam apple, dark skin, painting, earring, scratches, stains, old photo damages, bad quality, blurry, (beard:2), (facial hair:2)\n",
      "clip": [
        "14",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "8": {
    "inputs": {
      "samples": [
        "3",
        0
      ],
      "vae": [
        "14",
        2
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "12": {
    "inputs": {
      "pixels": [
        "31",
        0
      ],
      "vae": [
        "14",
        2
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "14": {
    "inputs": {
      "ckpt_name": "RealVisXL_V5.0_Lightning_fp16.safetensors"
    },
    "class_type": "CheckpointLoaderSimple",
    "_meta": {
      "title": "Load Checkpoint"
    }
  },
  "15": {
    "inputs": {
      "strength": [
        "61",
        0
      ],
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "6",
        0
      ],
      "negative": [
        "7",
        0
      ],
      "control_net": [
        "16",
        0
      ],
      "image": [
        "26",
        0
      ],
      "vae": [
        "14",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "16": {
    "inputs": {
      "type": "auto",
      "control_net": [
        "17",
        0
      ]
    },
    "class_type": "SetUnionControlNetType",
    "_meta": {
      "title": "SetUnionControlNetType"
    }
  },
  "17": {
    "inputs": {
      "control_net_name": "controlnet-union-sdxl-1.0-promax.safetensors"
    },
    "class_type": "ControlNetLoader",
    "_meta": {
      "title": "Load ControlNet Model"
    }
  },
  "18": {
    "inputs": {
      "images": [
        "8",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "RESULT"
    }
  },
  "19": {
    "inputs": {
      "images": [
        "73",
        0
      ]
    },
    "class_type": "PreviewImage",
    "_meta": {
      "title": "ORIGINAL IMAGE"
    }
  },
  "22": {
    "inputs": {
      "colors": 256,
      "dither": "floyd-steinberg",
      "image": [
        "31",
        0
      ]
    },
    "class_type": "ImageQuantize",
    "_meta": {
      "title": "ImageQuantize"
    }
  },
  "24": {
    "inputs": {
      "strength": 0.21,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "15",
        0
      ],
      "negative": [
        "15",
        1
      ],
      "control_net": [
        "25",
        0
      ],
      "image": [
        "22",
        0
      ],
      "vae": [
        "14",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "Apply ControlNet"
    }
  },
  "25": {
    "inputs": {
      "type": "canny/lineart/anime_lineart/mlsd",
      "control_net": [
        "17",
        0
      ]
    },
    "class_type": "SetUnionControlNetType",
    "_meta": {
      "title": "SetUnionControlNetType"
    }
  },
  "26": {
    "inputs": {
      "blur": 2,
      "image": [
        "31",
        0
      ]
    },
    "class_type": "LayerFilter: GaussianBlurV2",
    "_meta": {
      "title": "ADD BLUR TO SOURCE IMAGE"
    }
  },
  "31": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "megapixels": 2,
      "image": [
        "64",
        0
      ]
    },
    "class_type": "ImageScaleToTotalPixels",
    "_meta": {
      "title": "ImageScaleToTotalPixels"
    }
  },
  "32": {
    "inputs": {
      "strength": 0.1,
      "start_percent": 0,
      "end_percent": 1,
      "positive": [
        "24",
        0
      ],
      "negative": [
        "24",
        1
      ],
      "control_net": [
        "33",
        0
      ],
      "image": [
        "34",
        0
      ],
      "vae": [
        "14",
        2
      ]
    },
    "class_type": "ControlNetApplyAdvanced",
    "_meta": {
      "title": "ControlNet"
    }
  },
  "33": {
    "inputs": {
      "type": "depth",
      "control_net": [
        "17",
        0
      ]
    },
    "class_type": "SetUnionControlNetType",
    "_meta": {
      "title": "SetUnionControlNetType"
    }
  },
  "34": {
    "inputs": {
      "ckpt_name": "depth_anything_v2_vitl.pth",
      "resolution": [
        "36",
        0
      ],
      "image": [
        "31",
        0
      ]
    },
    "class_type": "DepthAnythingV2Preprocessor",
    "_meta": {
      "title": "Depth Anything V2 - Relative"
    }
  },
  "35": {
    "inputs": {
      "coarse": "disable",
      "resolution": [
        "36",
        0
      ],
      "image": [
        "22",
        0
      ]
    },
    "class_type": "LineArtPreprocessor",
    "_meta": {
      "title": "Realistic Lineart"
    }
  },
  "36": {
    "inputs": {
      "image": [
        "31",
        0
      ]
    },
    "class_type": "easy imageSizeByLongerSide",
    "_meta": {
      "title": "ImageSize (LongerSide)"
    }
  },
  "60": {
    "inputs": {
      "action": "append",
      "tidy_tags": "yes",
      "text_a": "a realistic colored photograph of vietnamese man's portrait",
      "text_b": "__USER_PROMPT__",
      "text_c": ", professtional photography, bright natural ambient light, realistic texture"
    },
    "class_type": "StringFunction|pysssss",
    "_meta": {
      "title": "PROMPT"
    }
  },
  "61": {
    "inputs": {
      "value": 0.6
    },
    "class_type": "easy float",
    "_meta": {
      "title": "LIKELINESS LEVEL [0.0-1.0]"
    }
  },
  "62": {
    "inputs": {
      "conditioning": [
        "71",
        0
      ],
      "latent": [
        "66",
        0
      ]
    },
    "class_type": "ReferenceLatent",
    "_meta": {
      "title": "ReferenceLatent"
    }
  },
  "63": {
    "inputs": {
      "conditioning": [
        "71",
        0
      ]
    },
    "class_type": "ConditioningZeroOut",
    "_meta": {
      "title": "ConditioningZeroOut"
    }
  },
  "64": {
    "inputs": {
      "samples": [
        "72",
        0
      ],
      "vae": [
        "76",
        0
      ]
    },
    "class_type": "VAEDecode",
    "_meta": {
      "title": "VAE Decode"
    }
  },
  "65": {
    "inputs": {
      "anything": [
        "64",
        0
      ]
    },
    "class_type": "easy cleanGpuUsed",
    "_meta": {
      "title": "Clean VRAM Used"
    }
  },
  "66": {
    "inputs": {
      "pixels": [
        "74",
        0
      ],
      "vae": [
        "76",
        0
      ]
    },
    "class_type": "VAEEncode",
    "_meta": {
      "title": "VAE Encode"
    }
  },
  "67": {
    "inputs": {
      "width": [
        "68",
        0
      ],
      "height": [
        "68",
        1
      ],
      "batch_size": 1
    },
    "class_type": "EmptyLatentImage",
    "_meta": {
      "title": "Empty Latent Image"
    }
  },
  "68": {
    "inputs": {
      "image": [
        "73",
        0
      ]
    },
    "class_type": "GetImageSize",
    "_meta": {
      "title": "Get Image Size"
    }
  },
  "70": {
    "inputs": {
      "upscale_method": "nearest-exact",
      "megapixels": 2,
      "image": [
        "75",
        0
      ]
    },
    "class_type": "ImageScaleToTotalPixels",
    "_meta": {
      "title": "OUTPUT SIZE"
    }
  },
  "71": {
    "inputs": {
      "text": "restore the photo to the natural original undamaged condition, while keeping the details like facial features.\nRemove the dirts and noises and film grains.\nRemove all the white scratches and marks and stains.\nRestore the blur and discolorization.\nCorrect the color balance and overall exposure value.\n",
      "clip": [
        "77",
        1
      ]
    },
    "class_type": "CLIPTextEncode",
    "_meta": {
      "title": "CLIP Text Encode (Prompt)"
    }
  },
  "72": {
    "inputs": {
      "seed": "__SEED_2__",
      "steps": "__STEPS_2__",
      "cfg": "__CFG_2__",
      "sampler_name": "euler",
      "scheduler": "normal",
      "denoise": 1,
      "model": [
        "77",
        0
      ],
      "positive": [
        "80",
        0
      ],
      "negative": [
        "63",
        0
      ],
      "latent_image": [
        "67",
        0
      ]
    },
    "class_type": "KSampler",
    "_meta": {
      "title": "KSampler"
    }
  },
  "73": {
    "inputs": {
      "grain_power": 1,
      "grain_scale": 0.5,
      "grain_sat": 0,
      "image": [
        "74",
        0
      ]
    },
    "class_type": "LayerFilter: AddGrain",
    "_meta": {
      "title": "ORIGINAL DETAILS (MIN = 0.00)"
    }
  },
  "74": {
    "inputs": {
      "blur": 1,
      "image": [
        "70",
        0
      ]
    },
    "class_type": "LayerFilter: GaussianBlurV2",
    "_meta": {
      "title": "NEW TEXTURE (MIN = 0.00)"
    }
  },
  "75": {
    "inputs": {
      "image": "__IMAGE_FILENAME__"
    },
    "class_type": "LoadImage",
    "_meta": {
      "title": "Load Image"
    }
  },
  "76": {
    "inputs": {
      "vae_name": "ae.safetensors"
    },
    "class_type": "VAELoader",
    "_meta": {
      "title": "Load VAE"
    }
  },
  "77": {
    "inputs": {
      "lora_name": "FLUX.1-Turbo-Alpha.safetensors",
      "strength_model": 1,
      "strength_clip": 1,
      "model": [
        "79",
        0
      ],
      "clip": [
        "78",
        0
      ]
    },
    "class_type": "LoraLoader",
    "_meta": {
      "title": "Load LoRA"
    }
  },
  "78": {
    "inputs": {
      "model_type": "flux.1",
      "text_encoder1": "clip_l.safetensors",
      "text_encoder2": "t5xxl_fp8_e4m3fn.safetensors",
      "t5_min_length": 512
    },
    "class_type": "NunchakuTextEncoderLoaderV2",
    "_meta": {
      "title": "Nunchaku Text Encoder Loader V2"
    }
  },
  "79": {
    "inputs": {
      "unet_name": "flux1-dev-kontext_fp8_scaled.safetensors",
      "weight_dtype": "default"
    },
    "class_type": "UNETLoader",
    "_meta": {
      "title": "Load Diffusion Model"
    }
  },
  "80": {
    "inputs": {
      "guidance": "__GUIDANCE__",
      "conditioning": [
        "62",
        0
      ]
    },
    "class_type": "FluxGuidance",
    "_meta": {
      "title": "MODIFY GRADE (MIN = 1.0)"
    }
  }
}
